plugins:

  # PEFT-related acceleration
  peft:

    # quantization-releated acceleration
    # e.g., kernels for quantized base weights
    quantization: 

      # AutoGPTQ quantized base weights.
      auto_gptq:
        kernel: triton_v2
        from_quantized: True

      # bitsandbytes quantized base weights.
      # bitsandbytes:
      #   quant_type: nf4 # fp4 

      unsloth:

        # uncomment this if you want the direct integration
        # direct_integration:
        #   base_layer: auto_gptq, # bitsandbytes (is this neede?) <- ITEM 4
        #   kernel: triton_v2

        stack_on: auto_gptq
        # apply_on: bitsandbytes (this will supercede) <- ITEM 4
        fused_lora: True
        fast_loss: True
        fast_rsm_layernorm: True
        fast_rope_embeddings: True